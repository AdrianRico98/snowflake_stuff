{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "nricyq6raherlebkl7v5",
   "authorId": "4487983831522",
   "authorName": "ADRIANRICO98",
   "authorEmail": "adrian.rico.analytics@gmail.com",
   "sessionId": "50e3385a-807f-42e9-902f-7e4111bea957",
   "lastEditTime": 1744031364137
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9cf62c-4d11-4aec-a047-688ea56821ac",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "# Análisis predictivo de supervivencia en el Titanic utilizando Snowflake ML\n\nEste proyecto demuestra un ciclo completo de MLOps implementado en Snowflake, utilizando el conocido dataset del Titanic. El objetivo es predecir la supervivencia de los pasajeros utilizando características demográficas y del viaje.\n\nA través de este proyecto, mostraremos cómo:\n- Configurar y gestionar entornos de datos en Snowflake.\n- Importar y preprocesar datos utilizando Snowpark.\n- Desarrollar pipelines de transformación escalables.\n- Entrenar y optimizar modelos con Snowflake ML.\n- Evaluar modelos y registrarlos en un repositorio.\n- Implementar modelos para inferencia en producción.\n\nTodo el flujo de trabajo se ejecuta dentro del ecosistema de Snowflake, aprovechando su procesamiento distribuido y sus capacidades de ML integradas."
  },
  {
   "cell_type": "code",
   "id": "2112b7da-7833-4c3c-b8bd-669598053a80",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "# Snowpark y Snowflake\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col, lit, when, round, count, avg, cast\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import IntegerType, FloatType, StringType, BooleanType, LongType, DoubleType\n\n# Snowflake ML para modelado\nimport snowflake.ml.modeling.preprocessing as snowml_preprocessing\nimport snowflake.ml.modeling.preprocessing as snowml\nimport snowflake.ml.modeling.impute as snowml_impute\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.linear_model import LogisticRegression \nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.ensemble import RandomForestClassifier # por si se quiere experimentar con otros modelos\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\nfrom snowflake.ml.registry import Registry\n\n# Bibliotecas para visualización y análisis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nimport shap\n\n# Otras bibliotecas útiles\nimport json\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8f66af9-200d-4b5d-84d5-38c0a44bac56",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "# Obtenemos la sesión activa\nsession = snowpark.Session.get_active_session()\n\n# Configuramos el warehouse, database y schema\nsession.sql(\"USE WAREHOUSE TITANIC_ML_WH\").collect()\nsession.sql(\"USE DATABASE TITANIC_ML_DB\").collect()\nsession.sql(\"USE SCHEMA TITANIC_ML_SCHEMA\").collect()\n\n# Añadimos un query tag para análisis\nsession.query_tag = {\"origin\": \"titanic_ml_demo\", \n                     \"name\": \"titanic_survival_prediction\", \n                     \"version\": {\"major\": 1, \"minor\": 0},\n                     \"attributes\": {\"is_demo\": 1}}\n\n# Mostramos los detalles de la conexión\nprint('Conexión establecida con los siguientes parámetros:')\nprint(f'Usuario     : {session.get_current_user()}')\nprint(f'Rol         : {session.get_current_role()}')\nprint(f'Database    : {session.get_current_database()}')\nprint(f'Schema      : {session.get_current_schema()}')\nprint(f'Warehouse   : {session.get_current_warehouse()}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74c674df-2123-4a52-b6bc-7f481db3b9e0",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "# Nota: Asumimos que el archivo CSV ya ha sido cargado en el stage TITANIC_ASSETS\n\n# Crear un Snowpark DataFrame que cargue los datos desde el CSV del stage\ntitanic_df = session.read.options({\n    \"field_delimiter\": \",\",\n    \"field_optionally_enclosed_by\": '\"',\n    \"skip_header\": 0,\n    \"parse_header\": True,\n    \"infer_schema\": True\n}).csv(\"@TITANIC_ASSETS/titanic_data.csv\")\n\n# Mostrar las primeras filas del dataset\ntitanic_df.show(10)\ntitanic_df.describe() #observamos la distribución de las variables\n\n\nprint(titanic_df.columns) #tenemos las columnas entrecomilladas y hará el proceso de tratamiento de datos más tedioso\n# Creamos la funcion que nos permite lidiar con ello\ndef clean_column_names(df, chars_to_remove='\"', replace_with=''):\n    \"\"\"Elimina caracteres no deseados de los nombres de columnas de un Snowpark DataFrame\"\"\"\n    for colname in df.columns:\n        new_colname = colname\n        for char in chars_to_remove:\n            new_colname = new_colname.replace(char, replace_with)\n        if new_colname != colname:\n            df = df.with_column_renamed(colname, new_colname)\n    return df\ntitanic_df = clean_column_names(titanic_df)\n    \n#Modificamos los nulos que importamos por defecto del conjunto del stage\ntitanic_df = titanic_df.withColumn(\n        \"AGE\",\n        when(col(\"AGE\").is_null(), np.nan).otherwise(col(\"AGE\"))\n    )\n#titanic_df.dtypes #observamos que embarked es importado como un double por lo que hacemos un tipecasting a string\ntitanic_df= titanic_df.withColumn(\"EMBARKED_CAST\", col(\"EMBARKED\").cast(StringType(2)))\ntitanic_df = titanic_df.withColumn(\n        \"EMBARKED_CAST\",\n        when(col(\"EMBARKED_CAST\").is_null(), np.nan).otherwise(col(\"EMBARKED_CAST\"))\n    )\ntitanic_df = titanic_df.drop(\"EMBARKED_CAST\")\nprint(titanic_df.columns)\nprint(\"\\nGuardando el dataframe original en una tabla...\")\ntitanic_df.write.mode('overwrite').save_as_table('TITANIC_ORIGINAL')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b57792b-c74e-485f-9ad7-aef342be8d5c",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "# Estrategia de preprocesamiento de datos con Snowpark ML\n\nEl manejo de datos faltantes y la transformación de variables es crucial para el rendimiento de los modelos predictivos. En las celdas siguientes, demostraremos cómo utilizar los distintos imputadores y transformadores que ofrece Snowflake ML:\n\n- **SimpleImputer**: Para manejar valores faltantes en variables numéricas (como la edad) y categóricas (como el puerto de embarque).\n- **OrdinalEncoder**: Para transformar variables categóricas ordinales como la clase del pasajero.\n- **OneHotEncoder**: Para convertir variables categóricas nominales como el sexo y el puerto de embarque en formato numérico.\n- **StandardScaler**: Para normalizar variables numéricas como la tarifa.\n\nAunque mostraremos cada transformación de forma independiente para fines didácticos, posteriormente encapsularemos toda esta lógica en una pipeline de preprocesamiento completa. Esta pipeline podrá ser reutilizada para procesar nuevos datos de manera consistente y automatizada."
  },
  {
   "cell_type": "code",
   "id": "e3c55aa5-97cb-4494-8a8a-d5e9aba43687",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "#simple imputer de snowpark.ml\ntry:\n    frequent_imputer = snowml_impute.SimpleImputer(\n        input_cols=[\"EMBARKED\"],\n        output_cols=[\"EMBARKED_IMPUTED\"],\n        strategy=\"most_frequent\",\n        missing_values=np.nan\n    )\n    print(\"\\nSimpleImputer creado correctamente\")\n    transformed_titanic_df = frequent_imputer.fit(titanic_df).transform(titanic_df)\n    print(\"\\nSimpleImputer aplicado correctamente\")\n    \nexcept Exception as e:\n    print(f\"Error al crear y ejecutar el SimpleImputer: {str(e)}\")\n\ntry:\n    simple_imputer = snowml_impute.SimpleImputer(\n        input_cols=[\"AGE\"],\n        output_cols=[\"AGE_IMPUTED\"],\n        strategy=\"median\",\n        missing_values=np.nan\n    )\n    \n    # Aplicamos el imputer\n    print(\"\\nSimpleImputer creado correctamente\")\n    transformed_titanic_df = simple_imputer.fit(transformed_titanic_df).transform(transformed_titanic_df)\n    print(\"\\nSimpleImputer aplicado correctamente\")\n    \nexcept Exception as e:\n    print(f\"Error al crear y ejecutar el SimpleImputer: {str(e)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e477615a-7ccc-4df8-9d38-e91812afb30c",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#ordinal encoder de snowpark.ml\ntry:\n    # Ajustamos las categorías según los valores actuales\n    categories = {\n        \"PCLASS\": np.array([\"1\", \"2\", \"3\"]) #Es importante definir las categorias de la variable en funcion del tiempo de dato que tiene la variable, puedes comprobar el tipo con df.dtypes\n    }\n    ordinal_encoder = snowml.OrdinalEncoder(\n        input_cols=[\"PCLASS\"],\n        output_cols=[\"PCLASS_OE\"],\n        categories=categories\n    )\n    print(\"\\nOrdinalEncoder creado correctamente\")\n    transformed_titanic_df = ordinal_encoder.fit(transformed_titanic_df).transform(transformed_titanic_df)\n    print(\"\\nOrdinalEncoder aplicado correctamente\")\nexcept Exception as e:\n    print(f\"Error al crear y ejecutar el OrdinalEncoder: {str(e)}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4b69343-6def-4bee-8931-ffbd8df76b6f",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#onehot encoder de snowpark.ml\ntry:\n    onehot_encoder = snowml.OneHotEncoder (\n        input_cols=[\"SEX\",\"EMBARKED_IMPUTED\"],\n        output_cols=[\"SEX_OHE\",\"EMBARKED_OHE\"],\n        drop = \"first\",\n        handle_unknown='ignore'\n    )\n    print(\"\\nOnehotEncoder creado correctamente\")\n    transformed_titanic_df = onehot_encoder.fit(transformed_titanic_df).transform(transformed_titanic_df)\n    print(\"\\nOnehotEncoder aplicado correctamente\")\nexcept Exception as e:\n    print(f\"Error al crear y ejecutar el OrdinalEncoder: {str(e)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b7f477a-6692-4eab-ac37-aad904623451",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#estandarización de snowpark.ml\ntry:\n    standard_scaler = snowml.StandardScaler(\n        input_cols=[\"FARE\"],\n        output_cols=[\"FARE_SCALED\"],\n        with_mean=True,  \n        with_std=True  \n    )\n    print(\"\\nStandardScaler creado correctamente\")\n    transformed_titanic_df = standard_scaler.fit(transformed_titanic_df).transform(transformed_titanic_df)\n    print(\"\\nStandardScaler aplicado correctamente\")\nexcept Exception as e:\n    print(f\"Error al crear y ejecutar el StandardScaler: {str(e)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61dac41e-dcd5-4d20-99b1-11cb9c897536",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#KNN imputer de snowpark.ml, lo dejo en el código por motivos ilustrativos, no formará parte de la pipeline posterior\n# try:\n#     knn_imputer = snowml_impute.KNNImputer(\n#         input_cols=[\"PCLASS_OE\", \"FARE_SCALED\", \"SIBSP\", \"PARCH\"],\n#         output_cols=[\"AGE_IMPUTED\"],\n#         n_neighbors=7,\n#         weights=\"distance\",\n#         metric=\"nan_euclidean\",\n#         add_indicator=True\n#     )\n#     \n#     print(\"\\nKNNImputer creado correctamente\")\n#     transformed_titanic_df = knn_imputer.fit(transformed_titanic_df).transform(transformed_titanic_df)\n#     print(\"\\nKNNImputer aplicado correctamente\")\n#     \n#     null_count_before = transformed_titanic_df.filter(col(\"AGE\").is_null()).count()\n#     null_count_after = transformed_titanic_df.filter(col(\"AGE_IMPUTED\").is_null()).count()\n#     print(f\"Valores nulos en AGE antes: {null_count_before}\")\n#     print(f\"Valores nulos en AGE_IMPUTED después: {null_count_after}\")\n#     \n# except Exception as e:\n#     print(f\"Error al crear y ejecutar el KNNImputer: {str(e)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b18ef08-748c-4db4-ae24-535299187421",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "columnas_a_eliminar = [\"NAME\", \"TICKET\", \"PASSENGERID\",\"CABIN\",\"AGE\",\"FARE\",\"EMBARKED\",\"SEX\"]  \n\n# Verificamos que existan todas las columnas que intentamos eliminar\ncolumnas_existentes = transformed_titanic_df.columns\ncolumnas_a_eliminar_filtradas = [col for col in columnas_a_eliminar if col in columnas_existentes]\n\n# Eliminamos las columnas\ntransformed_titanic_df = transformed_titanic_df.drop(*columnas_a_eliminar_filtradas)\nprint(f\"Columnas eliminadas: {columnas_a_eliminar_filtradas}\")\n\n# Mostramos las columnas finales\nprint(\"\\nColumnas finales para modelado:\")\nprint(transformed_titanic_df.columns)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c03aeca-471f-469a-95f9-3215cc155f7d",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "# Pipeline de preprocesamiento: Automatización y reproducibilidad\n\nUna ventaja clave de utilizar Snowflake ML es la capacidad de encapsular todo el preprocesamiento en una pipeline unificada. Esto garantiza:\n\n1. **Reproducibilidad**: El mismo procesamiento se aplica a datos de entrenamiento y producción.\n2. **Eficiencia**: Las transformaciones se ejecutan de forma distribuida en el warehouse de Snowflake.\n3. **Mantenibilidad**: Es más fácil gestionar y actualizar un pipeline unificado que transformaciones independientes.\n4. **Portabilidad**: La pipeline puede ser serializada, guardada en un stage y reutilizada según sea necesario.\n\nEn esta sección, construimos una pipeline que integra todas las transformaciones anteriores en un solo flujo de trabajo y la guardamos para su uso futuro con nuevos datos. Esto es fundamental para implementar soluciones de ML robustas en entornos de producción."
  },
  {
   "cell_type": "code",
   "id": "f70c60e8-31d3-46e5-8c24-5c452cff60a0",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "print(\"\\nCreando la pipeline de preprocesamiento...\")\n\n# Crear la pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n        # 1. Imputación para variables con valores faltantes\n        (\n            \"embarked_imputer\",\n            snowml_impute.SimpleImputer(\n                input_cols=[\"EMBARKED\"],\n                output_cols=[\"EMBARKED_IMPUTED\"],\n                strategy=\"most_frequent\",\n                missing_values=np.nan\n            )\n        ),\n        (\n            \"age_imputer\",\n            snowml_impute.SimpleImputer(\n                input_cols=[\"AGE\"],\n                output_cols=[\"AGE_IMPUTED\"],\n                strategy=\"median\",\n                missing_values=np.nan\n            )\n        ),\n\n        (\n            \"ordinal_encoder\",\n            snowml.OrdinalEncoder(\n                input_cols=[\"PCLASS\"],\n                output_cols=[\"PCLASS_OE\"],\n                categories=categories\n            )\n        ),\n\n        (\n            \"onehot_encoder\",\n            snowml.OneHotEncoder(\n                input_cols=[\"SEX\", \"EMBARKED_IMPUTED\"],\n                output_cols=[\"SEX_OHE\", \"EMBARKED_OHE\"],\n                drop=\"first\",\n                handle_unknown='ignore'\n            )\n        ),\n\n        (\n            \"standard_scaler\",\n            snowml.StandardScaler(\n                input_cols=[\"FARE\"],\n                output_cols=[ \"FARE_SCALED\"],\n                with_mean=True,\n                with_std=True\n            )\n        )\n    ]\n)\n\n\nprint(\"\\nAjustando la pipeline a los datos...\")\nfitted_pipeline = preprocessing_pipeline.fit(titanic_df)\nprint(\"\\nTransformando los datos con la pipeline...\")\ntransformed_df = fitted_pipeline.transform(titanic_df)\nprint(\"\\nPrimeras filas del dataframe transformado:\")\ntransformed_df.show(5)\nprint(\"\\nGuardando la pipeline en un archivo joblib...\")\nPIPELINE_FILE = '/tmp/titanic_preprocessing_pipeline.joblib'\njoblib.dump(fitted_pipeline, PIPELINE_FILE)\nprint(\"\\nSubiendo la pipeline al stage de Snowflake...\")\nsession.file.put(PIPELINE_FILE, \"@TITANIC_ASSETS\", overwrite=True)\n\nprint(\"\\n¡Pipeline creada, guardada y aplicada con éxito!\")\nprint(\"La pipeline está disponible en: @TITANIC_ASSETS/titanic_preprocessing_pipeline.joblib\")\nprint(\"\\nGuardando los datos transformados en una tabla...\")\ntransformed_df.write.mode('overwrite').save_as_table('TITANIC_TRANSFORMED')\n\nprint(\"\\nDatos transformados guardados en la tabla: TITANIC_TRANSFORMED\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0dc9e592-1ada-4078-92e3-0a0173abac7b",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "#Una vez guardada, podemos aplicar la pipeline sobre titanic_original, obviando el código anterior\nprint(\"\\nCargando la pipeline de preprocesamiento...\")\nsession.file.get('@TITANIC_ASSETS/titanic_preprocessing_pipeline.joblib', '/tmp')\ntitanic_df = session.table(\"TITANIC_ORIGINAL\")\nloaded_pipeline = joblib.load('/tmp/titanic_preprocessing_pipeline.joblib')\nprint(\"Pipeline cargada correctamente\")\nprint(\"\\nAplicando la pipeline a los datos...\")\ntransformed_df = loaded_pipeline.transform(titanic_df)\nprint(\"\\nPrimeras filas del dataframe transformado:\")\ntransformed_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3db463c9-e11e-48eb-a012-76a85ffbb200",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "# Análisis exploratorio: Descubriendo patrones en los datos del Titanic\n\nEl análisis exploratorio de datos (EDA) es fundamental para comprender las relaciones entre variables y guiar el desarrollo del modelo. En el caso del Titanic, algunas tendencias bien documentadas incluyen:\n\n- **Género**: Las mujeres tuvieron tasas de supervivencia significativamente más altas que los hombres.\n- **Clase**: Los pasajeros de primera clase sobrevivieron en mayor proporción que los de tercera.\n- **Edad**: Los niños tuvieron prioridad en los botes salvavidas.\n- **Tamaño familiar**: Personas viajando solas o en familias muy numerosas tuvieron menor probabilidad de supervivencia.\n\nEl gráfico generado muestra estas relaciones y otras más, proporcionando una visión integral de los factores que influyeron en la supervivencia."
  },
  {
   "cell_type": "code",
   "id": "9204bbe4-e356-406f-ab63-76b9cc3e219c",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "warnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"viridis\")\n\n\ntransformed_df = session.table(\"TITANIC_TRANSFORMED\")\noriginal_df = session.table(\"TITANIC_ORIGINAL\")\ndf = transformed_df.to_pandas()\ndf_original = original_df.to_pandas()\n\nfig = plt.figure(figsize=(24, 24))\n\n# 1. Supervivencia\nplt.subplot(3, 3, 1)\nsurvival_counts = df['SURVIVED'].value_counts(normalize=True) * 100\nsurvival_abs = df['SURVIVED'].value_counts()\ncolors = ['#ff6666', '#5599ff']\nplt.pie(survival_counts, labels=[f'No ({survival_abs[0]})', f'Sí ({survival_abs[1]})'], \n        autopct='%1.1f%%', startangle=90, colors=colors, explode=(0.05, 0.1),\n        wedgeprops=dict(width=0.6, edgecolor='w'))\nplt.title('Distribución de Supervivencia', fontsize=16)\n\n# 2. Correlación\nplt.subplot(3, 3, 2)\nnumeric_cols = ['SURVIVED', 'PCLASS_OE', 'AGE_IMPUTED', 'FARE_SCALED', 'SIBSP', 'PARCH']\nfor col in df.columns:\n    if 'SEX_OHE' in col or 'EMBARKED_OHE' in col:\n        numeric_cols.append(col)\n\ncorr = df[numeric_cols].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap=cmap, linewidths=0.7, \n            vmin=-1, vmax=1, annot_kws={\"size\": 9})\nplt.title('Correlación entre Variables', fontsize=16)\n\n# 3. Scatter plot Edad-Tarifa-Supervivencia-Clase\nplt.subplot(3, 3, 3)\nscatter = plt.scatter(df['AGE_IMPUTED'], df['FARE_SCALED'],\n                      c=df['SURVIVED'].map({0: '#ff6666', 1: '#5599ff'}),\n                      s=df['PCLASS_OE'].map({0: 150, 1: 100, 2: 50}),\n                      alpha=0.7, edgecolors='w', linewidth=0.5)\n\nplt.xlabel('Edad Imputada')\nplt.ylabel('Tarifa Escalada')\nplt.title('Relación Edad-Tarifa-Supervivencia-Clase', fontsize=16)\n\nsurvival_legend = plt.legend(['No sobrevivió', 'Sobrevivió'], loc='upper right', title='Supervivencia')\nplt.gca().add_artist(survival_legend)\n\nclass_sizes = [150, 100, 50]\nclass_labels = ['1ª Clase', '2ª Clase', '3ª Clase']\nfor i, size in enumerate(class_sizes):\n    plt.scatter([], [], s=size, c='gray', alpha=0.7, edgecolors='w', linewidth=0.5, \n                label=class_labels[i])\nplt.legend(scatterpoints=1, title='Clase', loc='upper left')\n\n# 4. Violin plot de edad por género y supervivencia\nplt.subplot(3, 3, 4)\nif 'SEX_OHE_male' in df.columns:\n    df['GENDER'] = df['SEX_OHE_male'].map({1: 'Hombre', 0: 'Mujer'})\nelif 'SEX_OHE_female' in df.columns:\n    df['GENDER'] = df['SEX_OHE_female'].map({1: 'Mujer', 0: 'Hombre'})\nelse:\n    df['GENDER'] = df_original['SEX']\n\ndf['SURVIVED_LABEL'] = df['SURVIVED'].map({0: 'No', 1: 'Sí'})\nviolin = sns.violinplot(x='GENDER', y='AGE_IMPUTED', hue='SURVIVED_LABEL', \n                      data=df, palette=['#ff6666', '#5599ff'], \n                      split=True, inner='quart', linewidth=1)\nviolin.set_title('Distribución de Edad por Género y Supervivencia', fontsize=16)\nviolin.set_xlabel('Género')\nviolin.set_ylabel('Edad')\n\nfor gender in ['Hombre', 'Mujer']:\n    for survived in [0, 1]:\n        subset = df[(df['GENDER'] == gender) & (df['SURVIVED'] == survived)]\n        if not subset.empty:\n            x = 0 if gender == 'Hombre' else 1\n            x_offset = -0.2 if survived == 0 else 0.2\n            plt.annotate(f'{len(subset)}',\n                        xy=(x + x_offset, subset['AGE_IMPUTED'].median()),\n                        xytext=(0, 10), textcoords='offset points',\n                        ha='center', va='bottom', fontsize=9,\n                        bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.8))\n\n# 5. Heatmap supervivencia por clase y género\nplt.subplot(3, 3, 5)\nclass_gender_survival = pd.crosstab(\n    [df['PCLASS_OE'].map({0: '1ª Clase', 1: '2ª Clase', 2: '3ª Clase'}), df['GENDER']],\n    df['SURVIVED_LABEL'],\n    normalize='index'\n).reset_index()\n\nclass_gender_survival_pivot = class_gender_survival.pivot(\n    index='PCLASS_OE', columns='GENDER', values='Sí'\n)\n\nsns.heatmap(class_gender_survival_pivot, annot=True, fmt='.1%', cmap='RdBu_r',\n           linewidths=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Tasa de Supervivencia'})\nplt.title('Supervivencia por Clase y Género', fontsize=16)\n\n# 6. Análisis de título y supervivencia\nplt.subplot(3, 3, 6)\nif 'NAME' in df_original.columns:\n    df_original['TITLE'] = df_original['NAME'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    title_mapping = {\n        'Capt': 'Officer', 'Col': 'Officer', 'Major': 'Officer', 'Dr': 'Professional',\n        'Rev': 'Professional', 'Jonkheer': 'Royalty', 'Don': 'Royalty', \n        'Sir': 'Royalty', 'Lady': 'Royalty', 'the Countess': 'Royalty',\n        'Dona': 'Royalty', 'Mme': 'Mrs', 'Mlle': 'Miss', 'Ms': 'Miss'\n    }\n    df_original['TITLE'] = df_original['TITLE'].map(lambda x: title_mapping.get(x, x))\n    \n    title_survival = df_original.groupby('TITLE')['SURVIVED'].agg(['mean', 'count']).reset_index()\n    title_survival.columns = ['Título', 'Supervivencia', 'Cantidad']\n    title_survival = title_survival[title_survival['Cantidad'] > 5].sort_values('Supervivencia', ascending=False)\n    \n    bars = sns.barplot(x='Título', y='Supervivencia', data=title_survival, \n                    palette=sns.color_palette(\"RdBu_r\", len(title_survival)))\n    \n    for i, row in title_survival.iterrows():\n        plt.text(i, row['Supervivencia']+0.02, f\"{row['Supervivencia']*100:.1f}%\\n({row['Cantidad']})\", \n                ha='center', va='bottom', fontsize=9)\n    \n    plt.title('Supervivencia por Título', fontsize=16)\n    plt.xlabel('Título')\n    plt.ylabel('Tasa de Supervivencia')\n    plt.ylim(0, 1.1)\n    plt.xticks(rotation=45)\n\n# 7. Análisis de puerto, clase y supervivencia\nplt.subplot(3, 3, 7)\nembarked_cols = [col for col in df.columns if 'EMBARKED_OHE' in col]\n\nif embarked_cols:\n    df['EMBARKED'] = None\n    for col in embarked_cols:\n        port = col.replace('EMBARKED_OHE_', '')\n        df.loc[df[col] == 1, 'EMBARKED'] = port\n    \n    embarked_class_survival = pd.crosstab(\n        [df['EMBARKED'], df['PCLASS_OE'].map({0: '1ª', 1: '2ª', 2: '3ª'})],\n        df['SURVIVED'],\n        normalize='index'\n    )[1].unstack()\n    \n    sns.heatmap(embarked_class_survival, annot=True, fmt='.1%', cmap='RdBu_r',\n               linewidths=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Tasa de Supervivencia'})\n    plt.title('Supervivencia por Puerto y Clase', fontsize=16)\n\n# 8. Relación entre tamaño familiar y supervivencia\nplt.subplot(3, 3, 8)\ndf['FAMILY_SIZE'] = df['SIBSP'] + df['PARCH'] + 1\ndf['FAMILY_GROUP'] = pd.cut(df['FAMILY_SIZE'], \n                            bins=[0, 1, 2, 4, 11], \n                            labels=['Solo', 'Pequeña (2)', 'Mediana (3-4)', 'Grande (5+)'])\n\nfam_survival = df.groupby('FAMILY_GROUP')['SURVIVED'].agg(['mean', 'count']).reset_index()\nfam_survival.columns = ['Tamaño Familiar', 'Supervivencia', 'Cantidad']\n\nax = plt.subplot(3, 3, 8)\nfam_survival['se'] = np.sqrt((fam_survival['Supervivencia'] * (1 - fam_survival['Supervivencia'])) / fam_survival['Cantidad'])\nfam_survival['ci'] = fam_survival['se'] * 1.96\n\nbars = plt.bar(fam_survival['Tamaño Familiar'], fam_survival['Supervivencia'], \n              yerr=fam_survival['ci'], capsize=5, \n              color=sns.color_palette(\"viridis\", len(fam_survival)))\n\nfor i, bar in enumerate(bars):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03, \n            f\"{fam_survival.iloc[i]['Supervivencia']*100:.1f}%\\n({fam_survival.iloc[i]['Cantidad']})\", \n            ha='center', va='bottom', fontsize=10)\n\nplt.title('Supervivencia por Tamaño Familiar', fontsize=16)\nplt.xlabel('Grupo Familiar')\nplt.ylabel('Tasa de Supervivencia')\nplt.ylim(0, 1.1)\n\n# 9. Análisis de clase, edad y supervivencia\nplt.subplot(3, 3, 9)\ndf['AGE_GROUP'] = pd.cut(df['AGE_IMPUTED'], \n                        bins=[0, 12, 18, 35, 50, 100],\n                        labels=['Niño (<12)', 'Adolescente (12-18)', 'Joven (19-35)', 'Adulto (36-50)', 'Mayor (>50)'])\n\nage_class_survival = pd.crosstab(\n    [df['AGE_GROUP'], df['PCLASS_OE'].map({0: '1ª', 1: '2ª', 2: '3ª'})],\n    df['SURVIVED'],\n    normalize='index'\n)[1].unstack()\n\nsns.heatmap(age_class_survival, annot=True, fmt='.1%', cmap='RdBu_r',\n           linewidths=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Tasa de Supervivencia'})\nplt.title('Supervivencia por Grupo de Edad y Clase', fontsize=16)\n\nplt.tight_layout(pad=3.0)\nplt.subplots_adjust(top=0.93)\nplt.suptitle('ANÁLISIS EXPLORATORIO TITANIC', fontsize=24, y=0.98)\n\nplt.savefig('/tmp/titanic_eda_advanced.png', dpi=300, bbox_inches='tight')\ntry:\n    session.file.put('/tmp/titanic_eda_advanced.png', '@TITANIC_ASSETS', overwrite=True)\nexcept:\n    pass",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0d28feb7-3814-47be-a96c-b2c1e63091d7",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "# Modelado predictivo: Comparativa y optimización de modelos\n\nEn esta sección, desarrollaremos dos modelos de clasificación para predecir la supervivencia:\n\n1. **Modelo base**: Implementación directa de XGBoost con parámetros por defecto.\n2. **Modelo optimizado**: Versión mejorada mediante búsqueda de hiperparámetros con GridSearchCV.\n\nCompararemos ambos modelos utilizando métricas estándar:\n- **Exactitud (Accuracy)**: Proporción general de predicciones correctas.\n- **Puntuación F1 (F1 Score)**: Media armónica de precisión y exhaustividad.\n- **Área bajo la curva ROC (ROC AUC)**: Capacidad del modelo para distinguir entre clases.\n\nAmbos modelos serán registrados en el repositorio de modelos de Snowflake para:\n- Mantener un historial de versiones.\n- Documentar métricas de rendimiento.\n- Facilitar la implementación en producción.\n\nFinalmente, seleccionaremos el modelo con mejor rendimiento para la inferencia de nuevos datos."
  },
  {
   "cell_type": "code",
   "id": "6dd1e009-d771-4ec5-b3d4-4add78620b5c",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Cargamos los datos transformados\ntitanic_transformed_df = session.table(\"TITANIC_TRANSFORMED\")\nprint(\"Primeras filas del dataset transformado:\")\ntitanic_transformed_df.show(5)\ntitanic_transformed_df = clean_column_names(titanic_transformed_df)\n\n\nCATEGORICAL_COLUMNS = [\"PCLASS_OE\", \"SEX_OHE_MALE\", \"EMBARKED_OHE_Q\",\"EMBARKED_OHE_S\"]\nNUMERICAL_COLUMNS = [\"AGE_IMPUTED\", \"FARE_SCALED\", \"SIBSP\", \"PARCH\"]\nINPUT_COLUMNS = CATEGORICAL_COLUMNS + NUMERICAL_COLUMNS\n\n\n\nTARGET_COLUMN = [\"SURVIVED\"]\nOUTPUT_COLUMN = [\"PREDICTION\"]\nprint(\"\\nColumnas de entrada para el modelo:\")\nprint(INPUT_COLUMNS)\nprint(\"\\nColumna objetivo:\")\nprint(TARGET_COLUMN)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17737e80-e4ac-4dc4-a5dd-c44342e451d7",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "#Dividimos el conjunto de datos en entrenamiento y prueba\ntrain_df, test_df = titanic_transformed_df.random_split(weights=[0.8, 0.2], seed=42)\n\nprint(f\"Tamaño del conjunto de entrenamiento: {train_df.count()}\")\nprint(f\"Tamaño del conjunto de testeo: {test_df.count()}\")\nprint(\"\\nDistribución de supervivencia en el conjunto de entrenamiento:\")\ntrain_df.group_by(\"SURVIVED\").count().show()\nprint(\"\\nDistribución de supervivencia en el conjunto de testeo:\")\ntest_df.group_by(\"SURVIVED\").count().show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8b51d95-5dbd-4e54-8d6d-a1e17c813c40",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "# Entrenamos un Modelo Simple sin tuneo de hiperparámetros ni nada sofisticado \nxgb_classifier = XGBClassifier(\n    input_cols=INPUT_COLUMNS,\n    label_cols=TARGET_COLUMN,\n    output_cols=OUTPUT_COLUMN\n)\n\nprint(\"Entrenando el modelo XGBoost...\")\nxgb_classifier.fit(train_df)\nresult = xgb_classifier.predict(test_df)\n\n# Evaluar el modelo\naccuracy = accuracy_score(df=result, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PREDICTION\")\nf1 = f1_score(df=result, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PREDICTION\")\nroc_auc = roc_auc_score(\n    df=result, \n    y_true_col_names=\"SURVIVED\", \n    y_score_col_names=\"PREDICTION\", \n    average='macro'\n)\n\nprint(f\"\\nResultados del modelo simple:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\n\n# Mostrar matriz de confusión con parámetros ajustados\nconf_matrix = confusion_matrix(\n    df=result, \n    y_true_col_name=\"SURVIVED\",  \n    y_pred_col_name=\"PREDICTION\", \n    normalize=None\n)\nprint(\"\\nMatriz de Confusión:\")\nprint(conf_matrix)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa50f91d-d39d-4fc5-ba60-a61c3886cbee",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# Exprimentamos con gridsearch y validación cruzada para la optimización de hiperparámetros\nprint(\"Escalando el warehouse...\")\nsession.sql(\"ALTER WAREHOUSE TITANIC_ML_WH SET WAREHOUSE_SIZE=MEDIUM\").collect()\n\n\nparam_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"max_depth\": [3, 5, 7],\n    \"learning_rate\": [0.01, 0.1, 0.2]\n}\ngrid_search = GridSearchCV(\n    estimator=XGBClassifier(),\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    n_jobs=-1,  # Utilizar todos los procesadores disponibles\n    input_cols=INPUT_COLUMNS,\n    label_cols=TARGET_COLUMN,\n    output_cols=OUTPUT_COLUMN\n)\n\nprint(\"Iniciando búsqueda de hiperparámetros...\")\ngrid_search.fit(train_df)\nbest_params = grid_search.to_sklearn().best_params_\nbest_score = grid_search.to_sklearn().best_score_\nprint(f\"\\nMejores parámetros encontrados: {best_params}\")\nprint(f\"Mejor puntuación de validación cruzada: {best_score:.4f}\")\n\n\nsession.sql(\"ALTER WAREHOUSE TITANIC_ML_WH SET WAREHOUSE_SIZE=XSMALL\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd5dba51-0c68-49ae-83bd-3f4f0adba1c2",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "#Evaluamos el mejor modelo en el conjunto de testeo\nbest_model_results = grid_search.predict(test_df)\n\nbest_accuracy = accuracy_score(df=best_model_results, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PREDICTION\")\nbest_f1 = f1_score(df=best_model_results, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PREDICTION\")\nbest_roc_auc = roc_auc_score(\n    df=best_model_results, \n    y_true_col_names=\"SURVIVED\", \n    y_score_col_names=\"PREDICTION\",\n    average='macro'\n)\n\nprint(f\"\\nResultados del mejor modelo:\")\nprint(f\"Accuracy: {best_accuracy:.4f}\")\nprint(f\"F1 Score: {best_f1:.4f}\")\nprint(f\"ROC AUC: {best_roc_auc:.4f}\")\nbest_model_results_pd = best_model_results.to_pandas()\n\ncm = confusion_matrix(\n    df=best_model_results, \n    y_true_col_name=\"SURVIVED\", \n    y_pred_col_name=\"PREDICTION\",  \n    normalize=None\n)\nprint(cm)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12e778df-d82f-4596-a67e-44f5724ca5f6",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "#Guardamos ambas versiones del modelo con registry y log_model\ndb = session.get_current_database()\nschema = session.get_current_schema()\nregistry = Registry(session=session, database_name=db, schema_name=schema)\nmodel_name = \"TITANIC_SURVIVAL_PREDICTOR\"\n\n# Registrar el modelo simple (versión inicial)\nmodel_ver_initial = registry.log_model(\n    model_name=model_name,\n    version_name=\"V3\", #v3 y v4 porque ya hemos hecho este proceso 2 veces\n    model=xgb_classifier,\n    sample_input_data=test_df[INPUT_COLUMNS],\n    options={\"enable_explainability\": True}\n)\nmodel_ver_initial.set_metric(metric_name=\"accuracy\", value=accuracy)\nmodel_ver_initial.set_metric(metric_name=\"f1_score\", value=f1)\nmodel_ver_initial.set_metric(metric_name=\"roc_auc\", value=roc_auc)\nmodel_ver_initial.comment = \"Modelo inicial XGBoost para predecir supervivencia en el Titanic.\"\n\n# Registrar el mejor modelo de GridSearchCV\nmodel_ver_optimized = registry.log_model(\n    model_name=model_name,\n    version_name=\"V4\",\n    model=grid_search.to_sklearn().best_estimator_,\n    sample_input_data=test_df[INPUT_COLUMNS],\n    options={\"enable_explainability\": True}\n)\nmodel_ver_optimized.set_metric(metric_name=\"accuracy\", value=best_accuracy)\nmodel_ver_optimized.set_metric(metric_name=\"f1_score\", value=best_f1)\nmodel_ver_optimized.set_metric(metric_name=\"roc_auc\", value=best_roc_auc)\nmodel_ver_optimized.set_metric(metric_name=\"best_params\", value=str(best_params))\nmodel_ver_optimized.comment = \"Modelo XGBoost optimizado con GridSearchCV para predecir supervivencia en el Titanic.\"\n\n\nprint(\"\\nModelos registrados:\")\nregistry.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca6dfce4-2018-4b51-922a-e06c0e55b86b",
   "metadata": {
    "language": "python",
    "name": "cell20",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Utilizamos el modelo guardado para realizar inferencia (celda independiente una vez ya se han registrado los modelos)\ndb = session.get_current_database()\nschema = session.get_current_schema()\nregistry = Registry(session=session, database_name=db, schema_name=schema)\nmodel_name = \"TITANIC_SURVIVAL_PREDICTOR\"\ntrain_df, test_df = titanic_transformed_df.random_split(weights=[0.8, 0.2], seed=42)\noptimized_model = registry.get_model(model_name).version(\"V4\")\n\n\nsample_data = test_df.limit(5)\nprint(\"Datos de ejemplo para inferencia:\")\nsample_data.show()\n\n# Realizar predicciones\npredictions = optimized_model.run(sample_data, function_name=\"predict\")\npredictions = clean_column_names(predictions)\n    \nprint(\"\\nPredicciones:\")\npredictions.select(\"SURVIVED\", \"OUTPUT_FEATURE_0\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c1426b8-2310-4b1d-8ed6-388f961c91d1",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "# Aportamos explicabilidad del modelo\n\n# Generar explicaciones para algunas muestras\nexplanations = optimized_model.run(test_df.limit(50), function_name=\"explain\")\nprint(\"Explicaciones del modelo:\")\nexplanations.show(5)\nexplanations = clean_column_names(explanations)\nfeature_cols = [col + \"_explanation\" for col in INPUT_COLUMNS]\nexplanations_pd = explanations.select(feature_cols).to_pandas()\n\n\nshap_exp = shap._explanation.Explanation(\n    values=explanations_pd.values, \n    feature_names=INPUT_COLUMNS\n)\nplt.close('all')\nplt.figure(figsize=(5, 5))\nshap.plots.bar(shap_exp, show=False) \nplt.title('Importancia absoluta de las Características en la Predicción de Supervivencia')\nplt.tight_layout()\nplt.show() ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2a8e9fe-9324-4561-94aa-d3d1c3338763",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# Exportación del modelo para uso externo (también se puede hacer desde el stage)\nimport os\nmodel_export_dir = '/tmp/titanic_model'\nif not os.path.exists(model_export_dir):\n    os.makedirs(model_export_dir)\n\noptimized_model.export(model_export_dir)\nprint(f\"Modelo exportado a: {model_export_dir}\")\n\n# Cargar el modelo para verificar\nloaded_model = optimized_model.load()\nprint(\"Modelo cargado correctamente\")",
   "execution_count": null
  }
 ]
}